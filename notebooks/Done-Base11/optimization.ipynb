{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ee9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install pyspark library\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ed739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sparksession \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ee90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sparksession object and providing appName \n",
    "spark=SparkSession.builder.appName(\"optimization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05086a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "AirlineDF = spark.read.option(\"header\", \"true\").csv(\"/Users/krishnapratap/Desktop/partation/data/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4893",
   "metadata": {},
   "source": [
    "# use take() in place of collect() for reduce time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total no. of flight by Unique Carriers\n",
    "%time AirlineDF.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d269f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF.registerTempTable(\"AirlineTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25849dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to see all columns with datatype\n",
    "spark.sql(\"describe AirlineTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total no. of flight by Unique Carriers\n",
    "%time spark.sql(\"select UniqueCarrier,count(UniqueCarrier) from AirlineTable group by UniqueCarrier\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total columns in dataframe\n",
    "len(AirlineDF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9dd70",
   "metadata": {},
   "source": [
    "# Follow process for optimization\n",
    "\n",
    "# When data is huge otherwise not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bc85a",
   "metadata": {},
   "source": [
    "# use coalesce() in place of repartition() to reduce the no. of partition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44a11d",
   "metadata": {},
   "source": [
    "# Process:- 1 reduce no. of partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check how many partation in current AirlineDF dataframe\n",
    "AirlineDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Use repartition for reduce no. of partation\n",
    "#AirlineDF1 = AirlineDF.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce No. of partation from 22 to 4\n",
    "AirlineDF1 = AirlineDF.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab78022",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb986808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again Check processing time\n",
    "# calculate total no. of flight by Unique Carriers\n",
    "%time AirlineDF1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF1.registerTempTable(\"AirlineTable1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total no. of flight by Unique Carriers\n",
    "%time spark.sql(\"select UniqueCarrier,count(UniqueCarrier) from AirlineTable1 group by UniqueCarrier\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41613a36",
   "metadata": {},
   "source": [
    "# Apache Parquet is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON, supported by many data processing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8379b",
   "metadata": {},
   "source": [
    "# Process:- 2 write parquet file and create new dataframe for parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first write dataframe into parquet file\n",
    "AirlineDF1.write.parquet(\"parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create new dataframe from parquet file\n",
    "AirlineDF_Par = spark.read.parquet(\"parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF_Par.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0699132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again Check processing time created dataframe from parquet file\n",
    "# calculate total no. of flight by Unique Carriers\n",
    "%time AirlineDF_Par.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF_Par.registerTempTable(\"AirlineTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total no. of flight by Unique Carriers\n",
    "%time spark.sql(\"select UniqueCarrier,count(UniqueCarrier) from AirlineTable2 group by UniqueCarrier\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spark.sql(\"select rtrim(DestCityName) from AirlineTable2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spark.sql(\"select substring_index(DestCityName, ',',1) as DestCityName from AirlineTable2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8b419",
   "metadata": {},
   "source": [
    "# avoid PySpark UDFâ€™s and use Spark SQL built-in functions as these functions provide optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3021112",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Destination = AirlineDF.select(\"DestAirportID\", \"DestAirportSeqID\", \"Dest\", \n",
    "                                      \"DestCityName\",\"DestState\",\"DestStateName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Destination.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e043bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def destination(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\",\")\n",
    "    for x in arr:\n",
    "       resStr= arr[0]\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting python function to UDF\n",
    "destinationUDF = udf(lambda z: destination(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before udf apply\n",
    "Flight_Destination.select(\"Dest\",\"DestCityName\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4536933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when udf apply\n",
    "%time Flight_Destination.select(col(\"Dest\"),destinationUDF(col(\"DestCityName\")).alias(\"DestCityName\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dbb95",
   "metadata": {},
   "source": [
    "# Avoid UDF use spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Destination.registerTempTable(\"AirlineTable3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spark.sql(\"select Dest,substring_index(DestCityName, ',',1) as DestCityName from AirlineTable3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac1ba1",
   "metadata": {},
   "source": [
    "# broadcast variables are used to save the copy of data across all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c049d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, broadcast\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12739a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new = sc.broadcast([\"AA\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call brodcast by using value\n",
    "data = words_new.value # accessing the value stored in the broadcast in master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf832158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF_Par.select(\"Year\",\"UniqueCarrier\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without Broadcast variable on filter\n",
    "%time AirlineDF_Par.where((AirlineDF_Par['UniqueCarrier']).isin('AA')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast variable on filter\n",
    "%time AirlineDF_Par.where((AirlineDF_Par['UniqueCarrier']).isin(data)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cd134",
   "metadata": {},
   "source": [
    "#  accumulator variables are used for the information through associative and cummulative operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699018e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparkcontext\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a accumulator variable\n",
    "accum=sc.accumulator(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty variable\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73691d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create RDD\n",
    "RDD=sc.parallelize([10,20,30,40,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cba372",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.foreach(lambda x:accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f19f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83018f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.foreach(lambda x:accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253802e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4f4a4",
   "metadata": {},
   "source": [
    "# serialization in pyspark\n",
    "\n",
    "# Serialization is used for performance tuning on Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146493eb",
   "metadata": {},
   "source": [
    "PySpark supports custom serializers for transferring data; this can improve\n",
    "performance.\n",
    "\n",
    "By default, PySpark uses L{PickleSerializer} to serialize objects using Python's\n",
    "C{cPickle} serializer, which can serialize nearly any Python object.\n",
    "Other serializers, like L{MarshalSerializer}, support fewer datatypes but can be\n",
    "faster.\n",
    "\n",
    "The serializer is chosen when creating L{SparkContext}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ab60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.serializers import MarshalSerializer\n",
    "sc = SparkContext(\"local\", \"serialization app\", batchSize=2, serializer = MarshalSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfecbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext is sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc19918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "AirlineDF = spark.read.parquet(\"parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc42f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total no. of flight by Unique Carriers\n",
    "%time AirlineDF.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8937317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check no. of partition\n",
    "AirlineDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.serializers import PickleSerializer\n",
    "sc = SparkContext(\"local\", \"serialization app\", batchSize=2, serializer=PickleSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext is sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "AirlineDF = spark.read.parquet(\"parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total no. of flight by Unique Carriers\n",
    "%time AirlineDF.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a053d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check no. of partition\n",
    "AirlineDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0d695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7814d67b",
   "metadata": {},
   "source": [
    "# spark Parallelism\n",
    "\n",
    "spark.default.parallelism is the default number of partition set by spark which is by default 200. and if you want to increase the number of partition than you can apply the property\n",
    "\n",
    "\n",
    "One important way to increase parallelism of spark processing is to increase the number of executors on the cluster. However, knowing how the data should be distributed, so that the cluster can process data efficiently is extremely important. The secret to achieve this is partitioning in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b6625",
   "metadata": {},
   "source": [
    "# Note:- http://spark-configuration.luminousmen.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a738f",
   "metadata": {},
   "source": [
    "# for RDD use spark.default.parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74245345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sparksession object and providing appName \n",
    "spark0=SparkSession.builder.appName(\"optimization0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afa140",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark0.conf.set(\"spark.default.parallelism\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1 = spark0.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=sc1.textFile(\"/Users/krishnapratap/Desktop/partation/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34015848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words using flatMap\n",
    "rdd_word1 = rdd2.flatMap(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d67822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a paired-rdd\n",
    "rdd_pair1 = rdd_word1.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurence per word using reducebykey()\n",
    "rdd_reduce1 = rdd_pair1.reduceByKey(lambda x,y: x+y)\n",
    "%time rdd_reduce1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d1f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d4d35d",
   "metadata": {},
   "source": [
    "# for dataframe use spark.sql.shuffle.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sparksession object and providing appName \n",
    "spark1=SparkSession.builder.appName(\"optimization1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1.conf.set(\"spark.sql.shuffle.partitions\",50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "AirlineDF5 = spark1.read.option(\"header\", \"true\").csv(\"/Users/krishnapratap/Desktop/partation/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total rows in dataframe\n",
    "%time AirlineDF5.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c6d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
